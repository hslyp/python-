{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9a1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\vs\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7500 entries, 0 to 7499\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Home Ownership                7500 non-null   int64  \n",
      " 1   Annual Income                 7500 non-null   float64\n",
      " 2   Years in current job          7500 non-null   float64\n",
      " 3   Tax Liens                     7500 non-null   float64\n",
      " 4   Number of Open Accounts       7500 non-null   float64\n",
      " 5   Years of Credit History       7500 non-null   float64\n",
      " 6   Maximum Open Credit           7500 non-null   float64\n",
      " 7   Number of Credit Problems     7500 non-null   float64\n",
      " 8   Months since last delinquent  7500 non-null   float64\n",
      " 9   Bankruptcies                  7500 non-null   float64\n",
      " 10  Long Term                     7500 non-null   int64  \n",
      " 11  Current Loan Amount           7500 non-null   float64\n",
      " 12  Current Credit Balance        7500 non-null   float64\n",
      " 13  Monthly Debt                  7500 non-null   float64\n",
      " 14  Credit Score                  7500 non-null   float64\n",
      " 15  Credit Default                7500 non-null   int64  \n",
      " 16  Purpose_business loan         7500 non-null   int32  \n",
      " 17  Purpose_buy a car             7500 non-null   int32  \n",
      " 18  Purpose_buy house             7500 non-null   int32  \n",
      " 19  Purpose_debt consolidation    7500 non-null   int32  \n",
      " 20  Purpose_educational expenses  7500 non-null   int32  \n",
      " 21  Purpose_home improvements     7500 non-null   int32  \n",
      " 22  Purpose_major purchase        7500 non-null   int32  \n",
      " 23  Purpose_medical bills         7500 non-null   int32  \n",
      " 24  Purpose_moving                7500 non-null   int32  \n",
      " 25  Purpose_other                 7500 non-null   int32  \n",
      " 26  Purpose_renewable energy      7500 non-null   int32  \n",
      " 27  Purpose_small business        7500 non-null   int32  \n",
      " 28  Purpose_take a trip           7500 non-null   int32  \n",
      " 29  Purpose_vacation              7500 non-null   int32  \n",
      " 30  Purpose_wedding               7500 non-null   int32  \n",
      "dtypes: float64(13), int32(15), int64(3)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "# 先运行之前预处理好的代码\n",
    "import pandas as pd\n",
    "import pandas as pd    #用于数据处理和分析，可处理表格数据。\n",
    "import numpy as np     #用于数值计算，提供了高效的数组操作。\n",
    "import matplotlib.pyplot as plt    #用于绘制各种类型的图表\n",
    "import seaborn as sns   #基于matplotlib的高级绘图库，能绘制更美观的统计图形。\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    " # 设置中文字体（解决中文显示问题）\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows系统常用黑体字体\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 正常显示负号\n",
    "data = pd.read_csv('data.csv')    #读取数据\n",
    "\n",
    "\n",
    "# 先筛选字符串变量 \n",
    "discrete_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "# Home Ownership 标签编码\n",
    "home_ownership_mapping = {\n",
    "    'Own Home': 1,\n",
    "    'Rent': 2,\n",
    "    'Have Mortgage': 3,\n",
    "    'Home Mortgage': 4\n",
    "}\n",
    "data['Home Ownership'] = data['Home Ownership'].map(home_ownership_mapping)\n",
    "\n",
    "# Years in current job 标签编码\n",
    "years_in_job_mapping = {\n",
    "    '< 1 year': 1,\n",
    "    '1 year': 2,\n",
    "    '2 years': 3,\n",
    "    '3 years': 4,\n",
    "    '4 years': 5,\n",
    "    '5 years': 6,\n",
    "    '6 years': 7,\n",
    "    '7 years': 8,\n",
    "    '8 years': 9,\n",
    "    '9 years': 10,\n",
    "    '10+ years': 11\n",
    "}\n",
    "data['Years in current job'] = data['Years in current job'].map(years_in_job_mapping)\n",
    "\n",
    "# Purpose 独热编码，记得需要将bool类型转换为数值\n",
    "data = pd.get_dummies(data, columns=['Purpose'])\n",
    "data2 = pd.read_csv(\"data.csv\") # 重新读取数据，用来做列名对比\n",
    "list_final = [] # 新建一个空列表，用于存放独热编码后新增的特征名\n",
    "for i in data.columns:\n",
    "    if i not in data2.columns:\n",
    "       list_final.append(i) # 这里打印出来的就是独热编码后的特征名\n",
    "for i in list_final:\n",
    "    data[i] = data[i].astype(int) # 这里的i就是独热编码后的特征名\n",
    "\n",
    "\n",
    "\n",
    "# Term 0 - 1 映射\n",
    "term_mapping = {\n",
    "    'Short Term': 0,\n",
    "    'Long Term': 1\n",
    "}\n",
    "data['Term'] = data['Term'].map(term_mapping)\n",
    "data.rename(columns={'Term': 'Long Term'}, inplace=True) # 重命名列\n",
    "continuous_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()  #把筛选出来的列名转换成列表\n",
    " \n",
    " # 连续特征用中位数补全\n",
    "for feature in continuous_features:     \n",
    "    mode_value = data[feature].mode()[0]            #获取该列的众数。\n",
    "    data[feature].fillna(mode_value, inplace=True)          #用众数填充该列的缺失值，inplace=True表示直接在原数据上修改。\n",
    "\n",
    "# 最开始也说了 很多调参函数自带交叉验证，甚至是必选的参数，你如果想要不交叉反而实现起来会麻烦很多\n",
    "# 所以这里我们还是只划分一次数据集\n",
    "\n",
    "data.drop(columns=['Id'], inplace=True) # 删除 Loan ID 列\n",
    "data.info() # 查看数据集的信息，包括数据类型和缺失值情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c392ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(['Credit Default'], axis=1)  # 特征，axis=1表示按列删除\n",
    "y = data['Credit Default'] # 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e58a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(['Credit Default'], axis=1)  # 特征，axis=1表示按列删除\n",
    "y = data['Credit Default'] # 标签\n",
    "# 按照8:2划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80%训练集，20%测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57463a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 默认参数随机森林 (训练集 -> 测试集) ---\n",
      "训练与预测耗时: 0.7729 秒\n",
      "\n",
      "默认随机森林 在测试集上的分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.96      0.85      1059\n",
      "           1       0.76      0.30      0.43       441\n",
      "\n",
      "    accuracy                           0.77      1500\n",
      "   macro avg       0.77      0.63      0.64      1500\n",
      "weighted avg       0.77      0.77      0.73      1500\n",
      "\n",
      "默认随机森林 在测试集上的混淆矩阵：\n",
      "[[1018   41]\n",
      " [ 309  132]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #随机森林分类器\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # 用于评估分类器性能的指标\n",
    "from sklearn.metrics import classification_report, confusion_matrix #用于生成分类报告和混淆矩阵\n",
    "import warnings #用于忽略警告信息\n",
    "warnings.filterwarnings(\"ignore\") # 忽略所有警告信息\n",
    "# --- 1. 默认参数的随机森林 ---\n",
    "# 评估基准模型，这里确实不需要验证集\n",
    "print(\"--- 1. 默认参数随机森林 (训练集 -> 测试集) ---\")\n",
    "import time # 这里介绍一个新的库，time库，主要用于时间相关的操作，因为调参需要很长时间，记录下会帮助后人知道大概的时长\n",
    "start_time = time.time() # 记录开始时间\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train) # 在训练集上训练\n",
    "rf_pred = rf_model.predict(X_test) # 在测试集上预测\n",
    "end_time = time.time() # 记录结束时间\n",
    "\n",
    "print(f\"训练与预测耗时: {end_time - start_time:.4f} 秒\")\n",
    "print(\"\\n默认随机森林 在测试集上的分类报告：\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"默认随机森林 在测试集上的混淆矩阵：\")\n",
    "print(confusion_matrix(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9da5ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting umap-learn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/8f/671c0e1f2572ba625cbcc1faeba9435e00330c3d6962858711445cf1e817/umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from umap-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from umap-learn) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from umap-learn) (0.60.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d2/53/d23a97e0a2c690d40b165d1062e2c4ccc796be458a1ce59f6ba030434663/pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\vs\\lib\\site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\envs\\vs\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\vs\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install umap-learn -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b2252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保这些库已导入，你的原始代码中可能已经包含了部分\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler # 特征缩放\n",
    "from sklearn.decomposition import PCA # 主成分分析\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA # 线性判别分析\n",
    "# UMAP 需要单独安装: pip install umap-learn\n",
    "import umap # 如果安装了 umap-learn，可以这样导入\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 你的 X_train, X_test, y_train, y_test 应该已经根据你的代码准备好了\n",
    "# 我们假设你的随机森林模型参数与基准模型一致，以便比较降维效果\n",
    "# rf_params = {'random_state': 42} # 如果你的基准模型有其他参数，也在这里定义\n",
    "# 为了直接比较，我们使用默认的 RandomForestClassifier 参数，除了 random_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f154fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. PCA 降维 + 随机森林 (不使用 Pipeline) ---\n",
      "为了保留95%的方差，需要的主成分数量: 26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import numpy as np # 确保numpy导入\n",
    "\n",
    "# 假设 X_train, X_test, y_train, y_test 已经准备好了\n",
    "\n",
    "print(f\"\\n--- 2. PCA 降维 + 随机森林 (不使用 Pipeline) ---\")\n",
    "\n",
    "\n",
    "# 步骤 1: 特征缩放\n",
    "scaler_pca = StandardScaler()\n",
    "X_train_scaled_pca = scaler_pca.fit_transform(X_train)\n",
    "X_test_scaled_pca = scaler_pca.transform(X_test) # 使用在训练集上fit的scaler\n",
    "\n",
    "# 步骤 2: PCA降维\n",
    "# 选择降到10维，或者你可以根据解释方差来选择，例如：\n",
    "pca_expl = PCA(random_state=42)\n",
    "pca_expl.fit(X_train_scaled_pca)\n",
    "cumsum_variance = np.cumsum(pca_expl.explained_variance_ratio_)\n",
    "n_components_to_keep_95_var = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "print(f\"为了保留95%的方差，需要的主成分数量: {n_components_to_keep_95_var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6b0beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA降维后，训练集形状: (6000, 10), 测试集形状: (1500, 10)\n",
      "手动PCA降维后，训练与预测耗时: 1.3917 秒\n",
      "\n",
      "手动 PCA + 随机森林 在测试集上的分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      1059\n",
      "           1       0.70      0.32      0.44       441\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.73      0.63      0.64      1500\n",
      "weighted avg       0.75      0.76      0.73      1500\n",
      "\n",
      "手动 PCA + 随机森林 在测试集上的混淆矩阵：\n",
      "[[997  62]\n",
      " [298 143]]\n"
     ]
    }
   ],
   "source": [
    "# 我们测试下降低到10维的效果\n",
    "n_components_pca = 10\n",
    "pca_manual = PCA(n_components=n_components_pca, random_state=42)\n",
    "\n",
    "X_train_pca = pca_manual.fit_transform(X_train_scaled_pca)\n",
    "X_test_pca = pca_manual.transform(X_test_scaled_pca) # 使用在训练集上fit的pca\n",
    "\n",
    "print(f\"PCA降维后，训练集形状: {X_train_pca.shape}, 测试集形状: {X_test_pca.shape}\")\n",
    "start_time_pca_manual = time.time()\n",
    "# 步骤 3: 训练随机森林分类器\n",
    "rf_model_pca = RandomForestClassifier(random_state=42)\n",
    "rf_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# 步骤 4: 在测试集上预测\n",
    "rf_pred_pca_manual = rf_model_pca.predict(X_test_pca)\n",
    "end_time_pca_manual = time.time()\n",
    "\n",
    "print(f\"手动PCA降维后，训练与预测耗时: {end_time_pca_manual - start_time_pca_manual:.4f} 秒\")\n",
    "\n",
    "print(\"\\n手动 PCA + 随机森林 在测试集上的分类报告：\")\n",
    "print(classification_report(y_test, rf_pred_pca_manual))\n",
    "print(\"手动 PCA + 随机森林 在测试集上的混淆矩阵：\")\n",
    "print(confusion_matrix(y_test, rf_pred_pca_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. t-SNE 降维 + 随机森林  ---\n",
      "       标准 t-SNE 主要用于可视化，直接用于分类器输入可能效果不佳。\n",
      "正在对训练集进行 t-SNE fit_transform...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # 用于可选的可视化\n",
    "import seaborn as sns # 用于可选的可视化\n",
    "\n",
    "# 假设 X_train, X_test, y_train, y_test 已经准备好了\n",
    "# 并且你的 X_train, X_test 是DataFrame或Numpy Array\n",
    "\n",
    "print(f\"\\n--- 3. t-SNE 降维 + 随机森林  ---\")\n",
    "print(\"       标准 t-SNE 主要用于可视化，直接用于分类器输入可能效果不佳。\")\n",
    "\n",
    "\n",
    "# 步骤 1: 特征缩放\n",
    "scaler_tsne = StandardScaler()\n",
    "X_train_scaled_tsne = scaler_tsne.fit_transform(X_train)\n",
    "X_test_scaled_tsne = scaler_tsne.transform(X_test) # 使用在训练集上fit的scaler\n",
    "\n",
    "# 步骤 2: t-SNE 降维\n",
    "# 我们将降维到与PCA相同的维度（例如10维）或者一个适合分类的较低维度。\n",
    "# t-SNE通常用于2D/3D可视化，但也可以降到更高维度。\n",
    "# 然而，降到与PCA一样的维度（比如10维）对于t-SNE来说可能不是其优势所在，\n",
    "# 并且计算成本会显著增加，因为高维t-SNE的优化更困难。\n",
    "# 为了与PCA的 n_components=10 对比，我们这里也尝试降到10维。\n",
    "# 但请注意，这可能非常耗时，且效果不一定好。\n",
    "# 通常如果用t-SNE做分类的预处理（不常见），可能会选择非常低的维度（如2或3）。\n",
    "\n",
    "# n_components_tsne = 10 # 与PCA的例子保持一致，但计算量会很大\n",
    "n_components_tsne = 2    # 更典型的t-SNE用于分类的维度，如果想快速看到结果\n",
    "                         # 如果你想严格对比PCA的10维，可以将这里改为10，但会很慢\n",
    "\n",
    "\n",
    "\n",
    "# 对训练集进行 fit_transform\n",
    "tsne_model_train = TSNE(n_components=n_components_tsne,\n",
    "                        perplexity=30,    # 常用的困惑度值\n",
    "                        n_iter=1000,      # 足够的迭代次数\n",
    "                        init='pca',       # 使用PCA初始化，通常更稳定\n",
    "                        learning_rate='auto', # 自动学习率 (sklearn >= 1.2)\n",
    "                        random_state=42,  # 保证结果可复现\n",
    "                        n_jobs=-1)        # 使用所有CPU核心\n",
    "print(\"正在对训练集进行 t-SNE fit_transform...\")\n",
    "start_tsne_fit_train = time.time()\n",
    "X_train_tsne = tsne_model_train.fit_transform(X_train_scaled_tsne)\n",
    "end_tsne_fit_train = time.time()\n",
    "print(f\"训练集 t-SNE fit_transform 完成，耗时: {end_tsne_fit_train - start_tsne_fit_train:.2f} 秒\")\n",
    "\n",
    "\n",
    "# 对测试集进行 fit_transform\n",
    "# 再次强调：这是独立于训练集的变换\n",
    "tsne_model_test = TSNE(n_components=n_components_tsne,\n",
    "                       perplexity=30,\n",
    "                       n_iter=1000,\n",
    "                       init='pca',\n",
    "                       learning_rate='auto',\n",
    "                       random_state=42, # 保持参数一致，但数据不同，结果也不同\n",
    "                       n_jobs=-1)\n",
    "print(\"正在对测试集进行 t-SNE fit_transform...\")\n",
    "start_tsne_fit_test = time.time()\n",
    "X_test_tsne = tsne_model_test.fit_transform(X_test_scaled_tsne) # 注意这里是 X_test_scaled_tsne\n",
    "end_tsne_fit_test = time.time()\n",
    "print(f\"测试集 t-SNE fit_transform 完成，耗时: {end_tsne_fit_test - start_tsne_fit_test:.2f} 秒\")\n",
    "\n",
    "print(f\"t-SNE降维后，训练集形状: {X_train_tsne.shape}, 测试集形状: {X_test_tsne.shape}\")\n",
    "\n",
    "start_time_tsne_rf = time.time()\n",
    "# 步骤 3: 训练随机森林分类器\n",
    "rf_model_tsne = RandomForestClassifier(random_state=42)\n",
    "rf_model_tsne.fit(X_train_tsne, y_train)\n",
    "\n",
    "# 步骤 4: 在测试集上预测\n",
    "rf_pred_tsne_manual = rf_model_tsne.predict(X_test_tsne)\n",
    "end_time_tsne_rf = time.time()\n",
    "\n",
    "print(f\"t-SNE降维数据上，随机森林训练与预测耗时: {end_time_tsne_rf - start_time_tsne_rf:.4f} 秒\")\n",
    "total_tsne_time = (end_tsne_fit_train - start_tsne_fit_train) + \\\n",
    "                  (end_tsne_fit_test - start_tsne_fit_test) + \\\n",
    "                  (end_time_tsne_rf - start_time_tsne_rf)\n",
    "print(f\"t-SNE 总耗时 (包括两次fit_transform和RF): {total_tsne_time:.2f} 秒\")\n",
    "\n",
    "\n",
    "print(\"\\n手动 t-SNE + 随机森林 在测试集上的分类报告：\")\n",
    "print(classification_report(y_test, rf_pred_tsne_manual))\n",
    "print(\"手动 t-SNE + 随机森林 在测试集上的混淆矩阵：\")\n",
    "print(confusion_matrix(y_test, rf_pred_tsne_manual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97422536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "# 假设你已经导入了 matplotlib 和 seaborn 用于绘图 (如果需要)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # 如果需要3D绘图\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n--- 4. LDA 降维 + 随机森林 ---\")\n",
    "\n",
    "# 步骤 1: 特征缩放\n",
    "scaler_lda = StandardScaler()\n",
    "X_train_scaled_lda = scaler_lda.fit_transform(X_train)\n",
    "X_test_scaled_lda = scaler_lda.transform(X_test) # 使用在训练集上fit的scaler\n",
    "\n",
    "# 步骤 2: LDA 降维\n",
    "n_features = X_train_scaled_lda.shape[1]\n",
    "if hasattr(y_train, 'nunique'):\n",
    "    n_classes = y_train.nunique()\n",
    "elif isinstance(y_train, np.ndarray):\n",
    "    n_classes = len(np.unique(y_train))\n",
    "else:\n",
    "    n_classes = len(set(y_train))\n",
    "\n",
    "max_lda_components = min(n_features, n_classes - 1)\n",
    "\n",
    "# 设置目标降维维度\n",
    "n_components_lda_target = 10\n",
    "\n",
    "if max_lda_components < 1:\n",
    "    print(f\"LDA 不适用，因为类别数 ({n_classes})太少，无法产生至少1个判别组件。\")\n",
    "    X_train_lda = X_train_scaled_lda.copy() # 使用缩放后的原始特征\n",
    "    X_test_lda = X_test_scaled_lda.copy()   # 使用缩放后的原始特征\n",
    "    actual_n_components_lda = n_features\n",
    "    print(\"将使用缩放后的原始特征进行后续操作。\")\n",
    "else:\n",
    "    # 实际使用的组件数不能超过LDA的上限，也不能超过我们的目标（如果目标更小）\n",
    "    actual_n_components_lda = min(n_components_lda_target, max_lda_components)\n",
    "    if actual_n_components_lda < 1: # 这种情况理论上不会发生，因为上面已经检查了 max_lda_components < 1\n",
    "        print(f\"计算得到的实际LDA组件数 ({actual_n_components_lda}) 小于1，LDA不适用。\")\n",
    "        X_train_lda = X_train_scaled_lda.copy()\n",
    "        X_test_lda = X_test_scaled_lda.copy()\n",
    "        actual_n_components_lda = n_features\n",
    "        print(\"将使用缩放后的原始特征进行后续操作。\")\n",
    "    else:\n",
    "        print(f\"原始特征数: {n_features}, 类别数: {n_classes}\")\n",
    "        print(f\"LDA 最多可降至 {max_lda_components} 维。\")\n",
    "        print(f\"目标降维维度: {n_components_lda_target} 维。\")\n",
    "        print(f\"本次 LDA 将实际降至 {actual_n_components_lda} 维。\")\n",
    "\n",
    "        lda_manual = LinearDiscriminantAnalysis(n_components=actual_n_components_lda, solver='svd')\n",
    "        X_train_lda = lda_manual.fit_transform(X_train_scaled_lda, y_train)\n",
    "        X_test_lda = lda_manual.transform(X_test_scaled_lda)\n",
    "\n",
    "print(f\"LDA降维后，训练集形状: {X_train_lda.shape}, 测试集形状: {X_test_lda.shape}\")\n",
    "\n",
    "start_time_lda_rf = time.time()\n",
    "# 步骤 3: 训练随机森林分类器\n",
    "rf_model_lda = RandomForestClassifier(random_state=42)\n",
    "rf_model_lda.fit(X_train_lda, y_train)\n",
    "\n",
    "# 步骤 4: 在测试集上预测\n",
    "rf_pred_lda_manual = rf_model_lda.predict(X_test_lda)\n",
    "end_time_lda_rf = time.time()\n",
    "\n",
    "print(f\"LDA降维数据上，随机森林训练与预测耗时: {end_time_lda_rf - start_time_lda_rf:.4f} 秒\")\n",
    "\n",
    "print(\"\\n手动 LDA + 随机森林 在测试集上的分类报告：\")\n",
    "print(classification_report(y_test, rf_pred_lda_manual))\n",
    "print(\"手动 LDA + 随机森林 在测试集上的混淆矩阵：\")\n",
    "print(confusion_matrix(y_test, rf_pred_lda_manual))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
